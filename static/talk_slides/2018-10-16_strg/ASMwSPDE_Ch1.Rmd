---
title: "Advanced Spatial Modeling with SPDEs using R and INLA"
subtitle: "Chapter 1: the Integrated Nested Laplace Approximation"
date: "2018/10/16"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
    css: [robot-fonts, robot]
---

$$\renewcommand{\vec}[1]{\boldsymbol{#1}}$$

```{r echo=FALSE}
knitr::opts_chunk$set(fig.width = 12, fig.align = 'center',
                      comment = NA,
                      cache = TRUE)
```

# Integrated Nested Laplace Approximation

For:

- observations $\vec{y}$ in an exponential family,
- depending on some hyperparameters $\vec{\theta_1}$,
- whose mean $\vec{\mu}$
- is linked to a linear predictor $\vec{\eta}$,
- potentially including some latent effects $\vec{x}$,
- where $\vec{x}$ are distributed as a GMRF,
- depending on some hyperparameters $\vec{\theta_2}$.

We have a likelihood

$$p(\vec{y} \mid \vec{x}, \vec{\theta}) = \prod_{i \in I} p(y_i \mid \eta_i, \vec{\theta}).$$

---

## INLA

We have a posterior

$$\begin{aligned}
p(\vec{x}, \vec{\theta} \mid \vec{y}) &\propto
  p(\vec{\theta}) \times p(\mathbf{x} \mid \vec{\theta}) \times \prod_{i \in I} p(y_i \mid x_i,\vec{theta})\\
  &\propto p(\vec{\theta}) \times \det\left[\vec{Q}(\vec{\theta})\right]^{1/2} \exp\left\{-\frac{1}{2}\vec{x}^\top \vec{Q}(\vec{\theta}) \vec{x}\right\} \times \prod_{i \in I} \exp \log p\left(y_i \mid \vec{x}, \vec{\theta}\right)\\
  &\propto p(\vec{\theta}) \times \det\left[\vec{Q}(\vec{\theta})\right]^{1/2} \exp\left\{-\frac{1}{2}\mathbf{x}^{\top} \vec{Q}(\vec{\theta}) \vec{x} + \sum_{i \in I} \log(p(y_i \mid x_i, \vec{\theta}))\right\}.
\end{aligned}$$

???

- Crappy MathJax rendering???
- See the latent GMRF field!

---

## INLA

Then we can get our posterior marginals as

$$\begin{aligned}
p(x_i \mid \vec{y}) &= \int p(x_i \mid \vec{\theta}, \vec{y}) p(\vec{\theta} \mid \vec{y}) d\vec{\theta}\\
p(\theta_j \mid \vec{y}) &= \int p(\vec{\theta} \mid \vec{y}) d\theta_{-j}
\end{aligned}$$

Both the densities and the integrals are approximated, so that

$$\tilde{p}(x_i \mid \vec{y}) = \sum_k \tilde{p}(x_i \mid \theta_k, \vec{y}) \cdot \tilde{p}(\theta_k \mid \vec{y}) \cdot \Delta_k.$$

Where $\Delta_k$ are integration weights.

???

- Various approximations are available

---

# R-INLA

```{r}
library(INLA)

data(SPDEtoy)
SPDEtoy.sp <- SPDEtoy
coordinates(SPDEtoy.sp) <- ~ s1 + s2
```

```{r echo=FALSE, messages=FALSE}
library(ggplot2)
library(viridis)
```

---

## A toy data set


```{r echo=FALSE}
ggplot(SPDEtoy, aes(x = s1, y = s2, color = y)) +
  geom_point(size = 5) +
  coord_fixed() +
  scale_color_viridis() +
  theme(axis.title = element_text(size = 16),
        axis.text = element_blank(),
        axis.ticks = element_blank(),
        legend.title = element_text(size = 20),
        legend.text = element_text(size = 16))
```

---

## A simple model

$$\begin{eqnarray}
y_i & \sim &  \operatorname{Normal}(\mu_i, \tau^{-1}), & i=1, \ldots,200 \nonumber \\
\mu_i & = & \alpha + \beta_1 s_{1i} + \beta_2 s_{2i} \nonumber \\
\alpha & \sim & \operatorname{Uniform} \nonumber\\
\beta_j & \sim & \operatorname{Normal}(0, 0.001^{-1}), & j = 1,2 \nonumber\\
\tau & \sim & \operatorname{Gamma}(1, 0.00005). \nonumber\\
\end{eqnarray}$$

---

## Fitting the simple model

```{r R.options=list(digits = 3)}
m0 <- inla(y ~ s1 + s2, data = SPDEtoy)
m0$summary.fixed
```

---

## Fitting the simple model

```{r echo = FALSE, fig.width = 12, fig.align='center'}
par(mfrow = c(2, 2), mar=c(3, 3, 1, 1), mgp=c(2,1,0),
    cex.lab = 2.5, cex.axis = 1.5)

# Fixed effects
plot(m0$marginals.fixed[[1]], type = "l",
     xlab = expression(alpha), ylab = "")
plot(m0$marginals.fixed[[2]], type = "l",
     xlab = expression(beta[1]), ylab = "")
abline(v = 0, lty = 3, col = rgb(0, 0, 0, 0.3))
plot(m0$marginals.fixed[[3]], type = "l",
     xlab = expression(beta[2]), ylab = "")
abline(v = 0, lty = 3, col = rgb(0, 0, 0, 0.3))
# Precision
plot(m0$marginals.hyperpar[[1]], type = "l", xlab = expression(tau), 
  ylab = "")
```

---

## Adding a random effect

Fit the effect of each coordinate as separable random walks, so that

$$\begin{aligned}
\Delta u_i &= u_i - u_{i+1} \sim N(0, \tau_{u}^{-1}), & i = 1,\ldots, n - 1\\
\mu_i &= \alpha + u_{1, (i)} + u_{2, (i')}
\end{aligned}$$

```{r}
f.rw1 <- y ~ f(s1, model = "rw1", scale.model = TRUE) +
             f(s2, model = "rw1", scale.model = TRUE)

m1 <- inla(f.rw1, data = SPDEtoy)
```

???

The `f` components in the formula indicate random effects.
---

### Side note: latent models

```{r}
names(inla.models()$latent)
```

---

## Adding a random effect

```{r R.options = list(digits = 3)}
m1$summary.fixed
```

```{r eval=FALSE}
m1$summary.hyperpar
```

```{r echo=FALSE}
m1_hyp <- m1$summary.hyperpar
rownames(m1_hyp) <- c("Prec obs", "Prec s1", "Prec s2")
m1_hyp
```

---

## Adding a random effect

```{r echo=FALSE}
par(mfrow = c(3, 2), mar = c(4, 4, 1, 1), mgp = c(2, 1, 0),
    cex.lab = 2.5, cex.axis = 1.5)

# Plot RW1 effect
# X: model$summary.random components to plot.
plot.rw1 <- function(X, ...) {
  plot(X[1:2], type = "l", 
       ylim=range(X[, c(4,6)]),  ...)
  lines(X$ID, X[,4], lty = 2)
  lines(X$ID, X[,6], lty = 2)
}

# Intercept
  plot(m1$marginals.fixed[[1]], type = "l", xlab = expression(alpha),
    ylab = "")
# Precision
plot(m1$marginals.hyperpar[[1]], type = "l", xlab = expression(tau),
  ylab = "")

# Non-linear effects
plot.rw1(m1$summary.random$s1, xlab = expression(s[1][",i"]),
  ylab = expression(u[1][",i"]))
abline(h = 0, lty = 3, col = rgb(0, 0, 0, 0.3))
plot.rw1(m1$summary.random$s2, xlab = expression(s[2][",i"]),
  ylab = expression(u[2][",i"]))
abline(h = 0, lty = 3, col = rgb(0, 0, 0, 0.3))
# Precisions on non-linear effects
plot(m1$marginals.hyperpar[[2]], type = "l", xlab = expression(tau[1]),
  ylab = "", xlim = c(0, 75))
plot(m1$marginals.hyperpar[[3]], type = "l", xlab = expression(tau[2]),
  ylab = "", xlim = c(0, 75))
```

---

## Prediction

```{r}
pred_locs <- expand.grid(s1 = seq(0.025, 0.975, len = 20),
                         s2 = seq(0.025, 0.975, len = 20),
                         y = NA)
SPDEtoy.pred <- rbind(SPDEtoy, pred_locs)
m0.pred <- inla(y ~ s1 + s2, data = SPDEtoy.pred,
                control.predictor = list(compute = TRUE))
m1.pred <- inla(f.rw1, data = SPDEtoy.pred,
                control.predictor = list(compute = TRUE))
```

???

Just use `NA`s as the response where you want predictions

---

## Prediction

```{r}
m0_exp_fn <- function(idx) {
    inla.emarginal(identity,
                   m0.pred$marginals.fitted.values[[idx]])
}
m0_exp_fitted <- sapply(201:600, m0_exp_fn)

m1_exp_fn <- function(idx) {
    inla.emarginal(identity,
                   m1.pred$marginals.fitted.values[[idx]])
}
m1_exp_fitted <- sapply(201:600, m1_exp_fn)
```

---

## Prediction

```{r echo=FALSE}
pred_df <- data.frame(rbind(pred_locs, pred_locs),
                      model = rep(c("Linear", "Random Walk"), each = 400),
                      Prediction = c(m0_exp_fitted, m1_exp_fitted))
ggplot(pred_df, aes(x = s1, y = s2, fill = Prediction, z = Prediction)) +
  geom_raster() +
  geom_contour(color = "white", alpha = 0.5) +
  scale_fill_viridis() +
  coord_fixed() +
  facet_wrap(~ model) +
  theme(axis.title = element_text(size = 24),
        strip.text = element_text(size = 20),
        axis.text = element_blank(),
        axis.ticks = element_blank(),
        legend.text = element_text(size = 16),
        legend.title = element_text(size = 20))
```

---

## Model and summary options

<table>
<caption><span id="tab:inlaopts">Table 1.3: </span> Some arguments taken by <code>inla()</code> to define a model and produce a summary of model fitting.</caption>
<colgroup>
<col width="14%" />
<col width="85%" />
</colgroup>
<thead>
<tr class="header">
<th>Argument</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>quantiles</code></td>
<td>Quantiles to be computed in the summary (default is <code>c(0.025, 0.5, 0.975)</code>).</td>
</tr>
<tr class="even">
<td><code>E</code></td>
<td>Expected values (for some Poisson models, default is <code>NULL</code>).</td>
</tr>
<tr class="odd">
<td><code>offset</code></td>
<td>Offset to be added to the linear predictor (default is <code>NULL</code>).</td>
</tr>
<tr class="even">
<td><code>weights</code></td>
<td>Weights on the observations (default is <code>NULL</code>)</td>
</tr>
<tr class="odd">
<td><code>Ntrials</code></td>
<td>Number of trials (for some Binomial models, default is <code>NULL</code>).</td>
</tr>
<tr class="even">
<td><code>verbose</code></td>
<td>Verbose output (default is <code>FALSE</code>).</td>
</tr>
</tbody>
</table>


---

## Model estimation control arguments

<table>
<caption><span id="tab:inlacontrol">Table 1.4: </span> Some arguments taken by <code>inla()</code> to customize the estimation process.</caption>
<colgroup>
<col width="26%" />
<col width="73%" />
</colgroup>
<thead>
<tr class="header">
<th>Argument</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>control.fixed</code></td>
<td>Control options for fixed effects.</td>
</tr>
<tr class="even">
<td><code>control.family</code></td>
<td>Control options for the likelihood.</td>
</tr>
<tr class="odd">
<td><code>control.compute</code></td>
<td>Control options for what is computed (e.g., DIC, WAIC, etc.)</td>
</tr>
<tr class="even">
<td><code>control.predictor</code></td>
<td>Control options for the linear predictor.</td>
</tr>
<tr class="odd">
<td><code>control.inla</code></td>
<td>Control options for how the posterior is computed.</td>
</tr>
<tr class="even">
<td><code>control.results</code></td>
<td>Control options for computing the marginals of random effects and linear predictors.</td>
</tr>
<tr class="odd">
<td><code>control.mode</code></td>
<td>Control options to set the modes of the hyperparameters.</td>
</tr>
</tbody>
</table>


---

## Model and summary options

```{r}
compute_opts <- list(dic = TRUE, cpo = TRUE, waic = TRUE)
m0.opts <- inla(y ~ s1 + s2, data = SPDEtoy,
                control.compute = compute_opts)
m1.opts <- inla(f.rw1, data = SPDEtoy,
                control.compute = compute_opts)
```

```{r include=FALSE}
opts_df <- data.frame(Model = c("Linear", "RW1"),
                      WAIC = c(m0.opts$waic$waic, m1.opts$waic$waic),
                      `Eff. par.` = c(m0.opts$waic$p.eff, m1.opts$waic$p.eff))
knitr::kable(opts_df)
```

| Model   |  WAIC | Effective parameters |
| :------ | ----: |           ---------: |
| Linear  |   810 |                 3.69 |
| RW1     |   800 |                15.53 |

---

## Approximation strategies

<table>
<caption><span id="tab:controlinla">Table 1.5: </span> Some options that can be passed though <code>control.inla</code> to control the estimation process with INLA. Check the manual page (with <code>?control.inla</code>) for more details.</caption>
<colgroup>
<col width="20%" />
<col width="80%" />
</colgroup>
<thead>
<tr class="header">
<th>Option</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>strategy</code></td>
<td>Strategy used for the approximations: <code>simplified.laplace</code> (default), <code>adaptive</code>, <code>gaussian</code> or <code>laplace</code>.</td>
</tr>
<tr class="even">
<td><code>int.strategy</code></td>
<td>Integration strategy: <code>auto</code> (default), <code>ccd</code>, <code>grid</code> or <code>eb</code> (check manual page for other options).</td>
</tr>
</tbody>
</table>

???

Strategies
- listed from least to most precise approximations
- also least to most computationally expensive

---

## Approximation strategies


```{r}
m1.ccd <- inla(f.rw1, data = SPDEtoy,
               control.compute = compute_opts,
               control.inla = list(int.strategy = "ccd"))
m1.grid <- inla(f.rw1, data = SPDEtoy,
                control.compute = compute_opts,
                control.inla = list(int.strategy = "grid"))
m1.eb <- inla(f.rw1, data = SPDEtoy,
              control.compute = compute_opts,
              control.inla = list(int.strategy = "eb"))
```

```{r include=FALSE}
m1.ccd$cpu.used
m1.grid$cpu.used
m1.eb$cpu.used

runtimes <- rbind(m1.ccd$cpu.used, m1.grid$cpu.used, m1.eb$cpu.used)
rownames(runtimes) <- c("CCD", "Grid", "EB")
knitr::kable(runtimes)
```

|     |   Pre| Running|  Post| Total|
|:----|-----:|-------:|-----:|-----:|
|CCD  | 0.327|   0.302| 0.042| 0.671|
|Grid | 0.334|   9.588| 0.060| 9.982|
|EB   | 0.319|   0.284| 0.044| 0.647|

???

CCD: Central Composite Design
Grid: grid integration
EB: Empirical Bayes; no integration of hyperparameters

Approximation precision:
grid > ccd > eb

Computation
grid >> ccd > eb

EB much reduced computation for more complicated models; maybe useful for quick model iteration?

---
class: left, middle

# It's not enough to say that you used a vague prior and hope.

.right[### Hope has no place in statistics.

—[Daniel Simpson](http://andrewgelman.com/2018/04/03/justify-my-love/)]

???
- Non-informative priors are *hard* (i.e. deriving Jeffrey's)
- Berger et al. 2001
- Oliveira 2007 with measurement error
- Model-specific
- Maybe just want MAP == MLE?
- What does that even mean?
- And besides that...

---

## Priors: Fixed effects

### Use the `control.fixed` argument

<table>
<caption><span id="tab:priorfixed">Table 1.6: </span> Options to set the prior of the fixed effects in argument <code>control.fixed</code>.</caption>
<colgroup>
<col width="24%" />
<col width="75%" />
</colgroup>
<thead>
<tr class="header">
<th>Option</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>mean.intercept</code></td>
<td>Prior mean for the intercept (default is <span class="math inline">\(0\)</span>).</td>
</tr>
<tr class="even">
<td><code>prec.intercept</code></td>
<td>Prior precision for the intercept (default is <span class="math inline">\(0\)</span>).</td>
</tr>
<tr class="odd">
<td><code>mean</code></td>
<td>Prior mean for the coefficients of the covariates (default is <span class="math inline">\(0\)</span>). It can be a named list.</td>
</tr>
<tr class="even">
<td><code>prec</code></td>
<td>Prior precision for the coefficients of the covariates (default is <span class="math inline">\(0.001\)</span>). It can be a named list.</td>
</tr>
</tbody>
</table>

???

- Note sure what's going on with intercept prior precision of 0? Probably a typo
- Separate priors for intercept and each covariate effect.
- Note priors on *precision*, not standard deviation

---

## Priors: Hyperparameters

### Use `hyper` in `control.likelihood`

<table>
<caption><span id="tab:priorhyper">Table 1.7: </span> Options to set the prior of the hyperparameters in the likelihood and random effects.</caption>
<colgroup>
<col width="24%" />
<col width="75%" />
</colgroup>
<thead>
<tr class="header">
<th>Option</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>initial</code></td>
<td>Initial value of the hyperparameter.</td>
</tr>
<tr class="even">
<td><code>prior</code></td>
<td>Prior distribution to be used.</td>
</tr>
<tr class="odd">
<td><code>param</code></td>
<td>Vector with the values of the parameters of the prior distribution.</td>
</tr>
<tr class="even">
<td><code>fixed</code></td>
<td>Boolean variable to set the parameter to a fixed value (default <code>FALSE</code>).</td>
</tr>
</tbody>
</table>

???

Be careful; priors are always on *internal* scale (e.g. log-precisions)

---

## Priors: Example

```{r}
# Prior on the fixed effects
prior.fixed <- list(mean.intercept = 0,
                    prec.intercept = 1,
                    mean = 0,
                    prec = 1)

# Prior on the likelihood precision (log-scale)
prior.prec <- list(initial = 0,
                   prior = "normal",
                   param = c(0, 1),
                   fixed = FALSE)

# Prior on the precision of the RW1
prior.rw1 <- list(initial = 0,
                  fixed = TRUE)
```

---

## Priors: Example

```{r}
f.hyper <- y ~ 1 +
  f(s1, model = "rw1",
    hyper = list(prec = prior.rw1),
    scale.model = TRUE) +
  f(s2, model = "rw1",
    hyper = list(prec = prior.rw1),
    scale.model = TRUE)

m1.hyper <- inla(f.hyper, data = SPDEtoy,
                 control.fixed = prior.fixed,
                 control.family =
                   list(hyper = list(prec = prior.prec)))
```

---

## Priors: Example

```{r}
m1$summary.fixed
m1.hyper$summary.fixed
```

---

## Priors: Example

```{r eval=FALSE}
m1$summary.hyperpar
```

```{r echo=FALSE}
m1_hyp
```

```{r eval=FALSE}
m1.hyper$summary.hyperpar
```

```{r echo=FALSE}
m1.hyp_hyp <- m1.hyper$summary.hyperpar
rownames(m1.hyp_hyp) <- c("Prec obs")
m1.hyp_hyp
```

???

RW1 hyperparameter values are not estimated (`fixed = TRUE`), so they are not listed in the summary.

---

## Using posterior marginals

<table>
<caption><span id="tab:fmarginals">Table 1.8: </span> Functions to manipulate the posterior marginals.</caption>
<thead>
<tr class="header">
<th>Function</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>inla.emarginal()</code></td>
<td>Compute the expectation of a function.</td>
</tr>
<tr class="even">
<td><code>inla.dmarginal()</code></td>
<td>Compute the density.</td>
</tr>
<tr class="odd">
<td><code>inla.pmarginal()</code></td>
<td>Compute a probability.</td>
</tr>
<tr class="even">
<td><code>inla.qmarginal()</code></td>
<td>Compute a quantile.</td>
</tr>
<tr class="odd">
<td><code>inla.rmarginal()</code></td>
<td>Sample from the marginal.</td>
</tr>
<tr class="even">
<td><code>inla.hpdmarginal()</code></td>
<td>Compute a high probability density (HPD) interval.</td>
</tr>
<tr class="odd">
<td><code>inla.smarginal()</code></td>
<td>Interpolate the posterior marginal.</td>
</tr>
<tr class="even">
<td><code>inla.mmarginal()</code></td>
<td>Compute the mode.</td>
</tr>
<tr class="odd">
<td><code>inla.tmarginal()</code></td>
<td>Transform the marginal.</td>
</tr>
<tr class="even">
<td><code>inla.zmarginal()</code></td>
<td>Compute summary statistics.</td>
</tr>
</tbody>
</table>

---

## Penalized Complexity priors

Shrinks a fit towards a "base" model, with interpretable priors:

$$\Pr(\sigma > u) = \alpha,\ u > 0,\ 0 < \alpha < 1.$$

So a prior on our random walk precision with a 1% probability of a precision less than 1 is declared as:

```{r}
pcprior <- list(prec = list(prior = "pc.prec",
                            param = c(1, 0.01)))
f.rw1.pc <- y ~
  f(s1, model = "rw1", scale.model = TRUE, hyper = pcprior) +
  f(s2, model = "rw1", scale.model = TRUE, hyper = pcprior)
m1.pc <- inla(f.rw1.pc, data = SPDEtoy)
```

---

## Penalized Complexity priors

```{r echo=FALSE}
## Fit with PC priors
post.sigma.s1 <- inla.tmarginal(function (x) sqrt(1 / exp(x)),
  m1.pc$internal.marginals.hyperpar[[2]])
post.sigma.s2 <- inla.tmarginal(function (x) sqrt(1 / exp(x)),
  m1.pc$internal.marginals.hyperpar[[3]])

## Fit with default priors
f.rw1 <- y ~ f(s1, model = "rw1", scale.model = TRUE) +
             f(s2, model = "rw1", scale.model = TRUE)
m1 <- inla(f.rw1, data = SPDEtoy)

#Transform to get the posterior of the st. dev.
post.sigma.s1.g <- inla.tmarginal (function (x) sqrt(1/exp(x)),
  m1$internal.marginals.hyperpar[[2]])
post.sigma.s2.g <- inla.tmarginal (function (x) sqrt(1/exp(x)),
  m1$internal.marginals.hyperpar[[3]])

post_x <- c(post.sigma.s1[, 1], post.sigma.s2[, 1],
            post.sigma.s1.g[, 1], post.sigma.s2.g[, 2])
post_dens <- c(post.sigma.s1[, 2], post.sigma.s2[, 2],
               post.sigma.s1.g[, 2], post.sigma.s2.g[, 2])

make_post_df <- function(post, par, prior) {
  data.frame(sigma = post[, 1],
             Density = post[, 2],
             par = par,
             Prior = prior)
}

post_sigma_df <- rbind(make_post_df(post.sigma.s1, "s1", "PC"),
                       make_post_df(post.sigma.s1.g, "s1", "Gamma"),
                       make_post_df(post.sigma.s2, "s2", "PC"),
                       make_post_df(post.sigma.s2.g, "s2", "Gamma"))

ggplot(post_sigma_df, aes(x = sigma, y = Density,
                         color = Prior, linetype = Prior)) +
  geom_line(size = 2) +
  facet_wrap(~ par) +
  theme(strip.text = element_text(size = 24),
        axis.title = element_text(size = 24),
        axis.text = element_text(size = 16),
        legend.title = element_text(size = 20),
        legend.text = element_text(size = 16))
```

???

Similar priors are also available for Matérn covariance parameters, as discussed in chapter 2.

---
class: left, middle

# Advanced likelihoods

---

## Multiple likelihoods

$$\begin{eqnarray}
y_i & \sim &  N(\mu_i, \tau_1^{-1}), & i = 1,\ldots, 200 \nonumber \\
y_i & \sim &  N(\mu_i,  \tau_2^{-1}), & i = 201,\ldots, 400 \nonumber \\
\mu_i & = & \alpha + \beta_1 s_{1i} + \beta_2 s_{2i}, & i=1,\ldots,400 \nonumber \\
\alpha & \sim & \operatorname{Uniform} \nonumber\\
\beta_j & \sim & N(0, 0.001^{-1}), & j = 1, 2 \nonumber\\
\tau_j & \sim & Ga(1, 0.00005), & j = 1, 2 \nonumber\\
\end{eqnarray}$$

---

## Multiple likelihoods

```{r}
# Generate new observations with additional noise
SPDEtoy$y2 <- SPDEtoy$y + rnorm(nrow(SPDEtoy), sd = 2)

# Number of locations
n <- nrow(SPDEtoy)

# Response matrix
Y <- matrix(NA, ncol = 2, nrow = n * 2)
# Add `y` in first column, rows 1 to 200
Y[1:n, 1] <- SPDEtoy$y
# Add `y2` in second column, rows 201 to 400
Y[n + 1:n, 2] <- SPDEtoy$y2
```

---

## Multiple likelihoods

```{r}
m0.2lik_dat <- data.frame(Y = Y,
                          s1 = rep(SPDEtoy$s1, 2),
                          s2 = rep(SPDEtoy$s2, 2))
m0.2lik <- inla(Y ~ s1 + s2,
                family = c("gaussian", "gaussian"),
                data = m0.2lik_dat)
```

```{r}
head(m0.2lik_dat, 3)
tail(m0.2lik_dat, 3)
```


???

Data frame is as expected, apparently INLA picks up a `Y` response to include `Y.1` and `Y.2`. Not sure if this means we should be careful about names in data frames passed to `inla`.

---

## Multiple likelihoods

```{r}
m0.2lik$summary.fixed
```

```{r eval=FALSE}
m0.2lik$summary.hyperpar
```

```{r echo=FALSE, R.options=list(digits = 3)}
m0.2lik_hyper <- m0.2lik$summary.hyperpar
rownames(m0.2lik_hyper) <- c("Prec obs 1", "Prec obs 2")
m0.2lik_hyper
```

---

## Copy effect

$$\begin{eqnarray}
  y_i & \sim &  N(\mu_i, \tau^{-1}), & i = 1,\ldots, 400 \nonumber \\
  \mu_i & = & \alpha + \beta_1 s_{1i} + \beta_2 s_{2i}, & i=1,\ldots,200 \nonumber \\
  \mu_i & = & \alpha + \beta_1 s_{1i} + \beta \cdot\beta^{*}_2 s_{2i}, & i=201,\ldots,400 \nonumber \\
  \alpha & \sim & \operatorname{Uniform} \nonumber\\
  \beta_j & \sim & N(0, 0.001^{-1}), & j = 1, 2 \nonumber\\
  \beta^*_2 & \sim & N(\beta_2, \tau_{\beta_2}^{-1} = 1 / \exp(14)) \nonumber \\
  \tau_j & \sim & Ga(1, 0.00005), & j = 1, 2 \nonumber
  \end{eqnarray}$$

???

- Share an effect between different parts of the likelihood
- Uses shared coefficient on s_2, with potential for some multiplicative error.

---

## Copy effect

```{r}
y.vec <- c(SPDEtoy$y, SPDEtoy$y2)
r <- rep(1:2, each = nrow(SPDEtoy))
s1.vec <- rep(SPDEtoy$s1, 2)
s2.vec <- rep(SPDEtoy$s2, 2)
i1 <- c(rep(1, n), rep(NA, n))
i2 <- c(rep(NA, n), rep(1, n))

d <- data.frame(y.vec, s1.vec, s2.vec, i1, i2)
```

---

## Copy effect

Simple copy:

```{r}
tau.prior = list(prec = list(initial = 0.001, fixed = TRUE))
f.copy <- y.vec ~ s1.vec +
  f(i1, s2.vec, model = "iid", hyper = tau.prior) +
  f(i2, s2.vec, copy = "i1")
m0.copy <- inla(f.copy, data = d)
```

With estimated multiplicative effect:

```{r}
f.copy2 <- y.vec ~ s1.vec + f(i1, s2.vec, model = "iid") +
                   f(i2, s2.vec, copy = "i1", fixed = FALSE)
m0.copy2 <- inla(f.copy2, data = d)
```

---

## Replicate effect

```{r}
d$r <- rep(1:2, each = nrow(SPDEtoy))
f.rep <- y.vec ~ f(s1.vec, model = "linear", replicate = r) +
                 f(s2.vec, model = "linear", replicate = r)
m0.rep <- inla(f.rep, data = d)
```

???

Shared hyperparameter values

---

## Linear combinations of latent effects

Uses the $\vec{A}$ "observation" matrix, where

$$\vec{\eta}^{*\top} = \vec{A}\vec{\eta}^\top.$$

```{r eval=FALSE}
# Define A matrix
A = Diagonal(n + n, 10)

# Fit model
m0.A <- inla(f.rep, data = d, control.predictor = list(A = A))
```

???

- Very useful for interpolating SPDE effects!
- This code does give an error though...

---

# Note!!

Most of the code, charts, and other content is taken from [Chapter 1 of ASMwSPDEinR&INLA](https://becarioprecario.bitbucket.io/spde-gitbook/ch-INLA.html), and all credit is due to those authors.
